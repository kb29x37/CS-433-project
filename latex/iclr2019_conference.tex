\documentclass{article} % For LaTeX2e
\usepackage{iclr2019_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}


\title{CS-433 final project}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Olivier Lemer, Louis Coulon}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Reproducibility of results obtained in research papers is an important part of
the research process, as it allows to support the conclusions of the paper, or
otherwise find potential inconsistencies in the results. The paper we have
chosen to reproduce is called \textit{MAE~: Mutual Posterior-Divergence
  Regularization for Variational Autoencoders}, and aims to improve the
performances of Variational Autoencoders (VAE) by constraining their latent
variables as to ensure they do contain useful information. VAEs are one of two
very powerful generative models and can be useful in many different fields,
which is why we decided to study this paper.
% The abstract paragraph should be indented 1/2~inch (3~picas) on both left and
% right-hand margins. Use 10~point type, with a vertical spacing of 11~points.
% The word \textsc{Abstract} must be centered, in small caps, and in point size 12. Two
% line spaces precede the abstract. The abstract must be limited to one
% paragraph.
\end{abstract}

\section {Introduction} % TODO do we need to change this to some other name ?
Our research history.

The paper \textit{MAE~: Mutual Posterior-Divergence Regularization for Variational
Autoencoders} is based on research on Variational Autoencoders (VAEs) and tries
to mitigate performance issues araising when the latent representation becomes
uninformative, collapsing the model to an unconditional generative model. We had
to read multiple papers about previous research on VAEs, as well as different
Neural Network models used in the studied paper. We will here introduce those
concepts and talk about the need for a better model than simple VAE as described
in the first paper introducing the model (\citet{vae}).

\subsection {Variational Autoencoders}
We learned about VAE, how it works

what's an autoencoder

An autoencoder is a generative model and can be decomposed into
two parts~: an \emph{encoder} and a \emph{decoder}, which both can be many kinds
of networks. The encoder generates a so-called \emph{latent representation} of
the original data received as input, which the decoder will then use to try and
recover the original information. The goal is then to train the encoder to
generate the most informative latent variables, and the decoder to reconstruct
the original data as closely as possible. Such a model can be very useful for
many reasons~: an efficient latent representation can be useful for analysis and
understanding of some natural process or for data representation tasks, and when
used along with the decoder, it can be used to generate realistic data from small
representations.

For the rest of our discussion, let $x$ be an observable random
variable which is assumed to be generated from a hidden continuous random
variable $z$, following a conditional distribution $p_\theta(x|z)$.

Kingma and Welling in their paper wish to design an autoencoder that works well
even on latent variables with intractable posterior distributions
$p_\theta(x|z)$, i.e. that cannot be evaluated or differenciated. To this end,
they introduce what they call a \emph{recognition
  model} $q_\phi(z|x)$, an approximation to the true posterior $p_\theta(z|x)$.
They also call $q_\phi(z|x)$ the \emph{encoder} and consequently the conditional
distribution $p_\theta(x|z)$ the \emph{decoder}.

In fact, they point out that the marginal likelihood $\log p_\theta(x)$ can be
lower bound by the following~:
\begin{align}\label{loss1}
  \log p_\theta(x)\geq \mathcal{L}(\theta, \phi; x)=-D_{KL}\left(
  q_\phi(z|x)||p_\theta(z) \right)+\mathbb{E}_{q_\phi(z|x)}\left[ \log p_\theta(x|z) \right]
\end{align}
which they explain has too high a variance to be practical when estimated using
a standard Monte Carlo gradient estimator, and where $D_{KL}$ is
the KL divergence of two distributions. To solve this problem, they introduce a
new lower bound that takes advantage of a \emph{parameterization trick}, which
consists in rewriting the distribution of $z$ as a function of a noise
variable $\varepsilon$~:
\begin{align}\label{reparam}
z\sim q_\phi(z|x)=g_\phi(\varepsilon|x)
\end{align}
where $\varepsilon$ follows an appropriate distribution $p(\varepsilon)$. This
makes it possible, after replacing the lower bound in 
\ref{loss1}, to apply a Monte Carlo
estimator on the second term, yielding~:
\begin{align}\label{loss2}
\tilde{\mathcal{L}}(\theta,\phi;x)=-D_{KL}\left(
  q_\phi(z|x)||p_\theta(z) \right)+\frac{1}{L}\sum_{l=1}^{L}\log p_\theta(x, g_\phi(\varepsilon_l,x))
\end{align}

where $L$ is some sample size and $\varepsilon_l$ is a sample from the
distribution of $\varepsilon$.

%Kingma and Welling in their paper (\citet{vae}) describe a problem that occurs
%when the marginal likelihood of some data $x$ is intractable, i.e. which cannot
%be evaluated or differenciated.
%The observation is the following~: when evaluating the quality of reconstructed
%data $x$, the lower bound $\mathcal{L}(\theta, \phi; x)$ on the marginal likelihood
%of this data, for 
%
%Kingma and Welling in their paper (\citet{vae}) wish to design an autoencoder
%that works well on continuous latent variables with intractable posterior
%distributions, i.e. for which the marginal likelihood $p_\theta(x)$ of some data
%$x$ with parameters $\theta$ is intractable, i.e. cannot be evaluated or
%differenciated, and when the dataset is too large for batch optimizations.
%
%
%
%wish to design an autoencoder
%that works well on continuous latent variables with intractable posterior
%distributions. 
%
%The paper from Kingma and Welling describe a way to reparameterize 

\subsection {Autoencoding capabilities of VAEs}
Read about VLAE, which explains well why VAE is not enough. Talk about
KL-varnishing at some point

As a followup reading, cited in our paper of interest about MAEs, we looked at
\textit{Variational Lossy Autoencoders} (\citet{vlae}), where we found good
insight as to why VAEs on their own are not always sufficient for autoencoding.

They note that when the decoder is made too expressive, it is easy for the model
to set the approximate posterior $q_{\phi}(z|x)$ to equal the prior
$p_{\theta}(z)$ and hence avoid any cost from the regularizing term
$D_{KL}(p_\phi(z|x)||p_\theta(z))$ in \ref{loss2}. Although this is explained by
``optimization challenges'' of VAE in most research
% (\citet{bowman})
, they present another observation that, even on a perfectly optimized VAE,
ignoring the latent variables should still result in optimum results in most
instances of VAE with intractable true posterior and powerful enough decoders.

%\subsection {Neural Networks} % TODO change the name of this
%Also read about ConvNet, ResNet, since talked about it in the paper

\subsection {ConvNet}
Finally, as most experiments in our paper of interest make use of Neural networks
for the VAE encoder and decoder, we describe here in essence what they are. Note
that we focus on Convolutional Neural Networks (ConvNet) rather than Residual
Neural Networks (ResNet), even though the latter is also used in the paper. We
unfortunately have had technical and timing constraints, and Resnet being
a fairly big model, we decided to focus on having good results on ConvNet alone.

\paragraph {Regular Neural Nets} A Neural Network is composed of a sequence of
hidden layers, each consisting of neurons which connect to all neurons of the
previous layer. The first layer takes a single vector as input, and the last
layer outputs the result. For example, in a classification problem, it outputs a
score for each class.

\paragraph {Convolutional Neural Nets}
An issue with Regular Neural Networks is the fact that each neuron must be
connect to all neurons of the previous layer, which makes the model
impractically big for large input data. Convolution Neural Networks (ConvNet)
take advantage of the fact that, in inputs that represent images, localized
information in the image can be enough for learning. Each layer organizes its
neurons in a three dimensions (width, height, depth) and each neuron is not
constrained to be fully connected to the previous layer. The first layer then
usually has the same dimensions as the input images, and for the example of a
classification problem with $c$ classes, the output data will have dimension
$1\times 1\times c$.

% Citer lossy, c'est bon point de base sur pouquoi c'est pas suffisant, ils
% expliquent bien
\section{Experiments}
\subsection{Our journey towards MAE implementation}
As a first step, we wanted to reproduce the VAE model. In order to do this we
used the pytorch framework. We generated an encoder decoder structure, with the
encode and the decoder as one layer 

\subsection{Experiment results}


\subsection{Evaluating our experiments}


\section {Conclusion}
Ouverture, future work.
\begin{itemize}
\item extend MAE to other forms of data, in particular text (on which VAEs
  suffer a more serious KL-varnishing problem).
\item Hypespherical Variational Auto-Encoders
  \href{https://arxiv.org/pdf/1804.00891.pdf}{link}. Use another distribution
  than Gaussian to get different latent space (hypespherical structured latent
  space), better suited for some types of data.
\end{itemize}


\bibliography{iclr2019_conference}
\bibliographystyle{iclr2019_conference}

\end{document}

